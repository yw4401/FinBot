{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting peft\n",
      "  Using cached peft-0.3.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (23.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (2.0.0)\n",
      "Requirement already satisfied: transformers in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (4.28.1)\n",
      "Requirement already satisfied: accelerate in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from peft) (0.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (4.5.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from transformers->peft) (0.14.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from transformers->peft) (2023.3.23)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from transformers->peft) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from transformers->peft) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.11.0->transformers->peft) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from requests->transformers->peft) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from requests->transformers->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from requests->transformers->peft) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from requests->transformers->peft) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/envs/vicuna/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Installing collected packages: peft\n",
      "Successfully installed peft-0.3.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install torch==1.13.1+cu116 torchvision==0.14.1+cu116 torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu116\n",
    "#%pip install transformers datasets evaluate rouge-score py7zr\n",
    "#%pip install nltk\n",
    "#%pip install accelerate\n",
    "#%pip install sentencepiece\n",
    "#%pip install bitsandbytes\n",
    "%pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import logging\n",
    "import pathlib\n",
    "from typing import Dict, Optional, Sequence\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from typing import List\n",
    "import transformers\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"/home/jupyter/koala_transformer\"\n",
    "#data_path = \"/home/jupyter/fine_tune.json\"\n",
    "data_path = \"/home/jupyter/FinBot/test data/finetunetest.json\"\n",
    "#cache_dir = \"/home/jupyter/data/transformers\"\n",
    "output_dir = \"/home/jupyter/testkoala\"\n",
    "\n",
    "optimizer = \"adamw_torch\"\n",
    "max_seq_length = 1024\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_BOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"</s>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def smart_tokenizer_and_embedding_resize(\n",
    "    special_tokens_dict: Dict,\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    "    model: transformers.PreTrainedModel,\n",
    "):\n",
    "    \"\"\"Resize tokenizer and embedding.\n",
    "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
    "\n",
    "    Note from Shilong: this function seems to create a pad token when the original model/tokenizer doesn't support it.\n",
    "    In particular, it adds the pad tokens to the tokenizer's token dictionary so that it can parse the token properly. Then,\n",
    "    for the actual tensor input that correspond to the token that goes into the model (i.e. the token embeddings),\n",
    "    it fills it with the average of all known token embeddings.\n",
    "    \"\"\"\n",
    "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if num_new_tokens > 0:\n",
    "        input_embeddings = model.get_input_embeddings().weight.data\n",
    "        output_embeddings = model.get_output_embeddings().weight.data\n",
    "\n",
    "        input_embeddings_avg = input_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "        output_embeddings_avg = output_embeddings[:-num_new_tokens].mean(\n",
    "            dim=0, keepdim=True)\n",
    "\n",
    "        input_embeddings[-num_new_tokens:] = input_embeddings_avg\n",
    "        output_embeddings[-num_new_tokens:] = output_embeddings_avg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "system = \"\"\"BEGINNING OF CONVERSATION: \"\"\"\n",
    "human = \"\"\n",
    "\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str],\n",
    "                 tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    \"\"\"Tokenize a list of strings.\n",
    "       Note from Shilong: this seems to mostly be about converting the strings into token ids as required by\n",
    "       hugging face library.\n",
    "    \"\"\"\n",
    "\n",
    "    # Note from Shilong: They truncate the texts here due to the model input size constraint.\n",
    "    # In the future, we may need to come up with a smarter way of doing this rather than truncation.\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "\n",
    "    # Note from Shilong: Since the task is language modeling, i.e. predicting the next token from the previous ones,\n",
    "    # the inputs are also the labels in this case.\n",
    "    input_ids = labels = [\n",
    "        tokenized.input_ids[0] for tokenized in tokenized_list\n",
    "    ]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "\n",
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "    \"\"\"Add speaker and start/end signal on each round.\n",
    "       Note from Shilong: I changed this from version 0 of their model to version 1, which\n",
    "       is what we tried.\n",
    "    \"\"\"\n",
    "    conversation = header\n",
    "    unknown_role = \"UNKNOWN\"  # use default unknown role\n",
    "    roles = {\n",
    "        \"human\": \"USER\",  # human role\n",
    "        \"gpt\": \"GPT\",  # gpt role\n",
    "    }\n",
    "    seperator = {\n",
    "        \"human\": \" \",\n",
    "        \"gpt\": DEFAULT_EOS_TOKEN\n",
    "    }\n",
    "    for sentence in source:\n",
    "        sentence_from = sentence[\"from\"].lower()\n",
    "        sentence[\"value\"] = (\n",
    "            roles.get(sentence_from, unknown_role)\n",
    "            + \": \"\n",
    "            + sentence[\"value\"]\n",
    "            + seperator[sentence_from]\n",
    "        )\n",
    "        if get_conversation:\n",
    "            conversation += sentence[\"value\"]\n",
    "    return conversation\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token_id = (0)\n",
    "tokenizer.padding_side=\"left\"\n",
    "   \n",
    "\n",
    "def _mask_targets(target, tokenized_lens, speakers, header_len, s_ids):\n",
    "    \"\"\"\n",
    "    Note from Shilong: This function just change the token id of the prompt to IGNORE so that\n",
    "    they don't count in the loss function. TODO: again, need to verify if it is correct.\n",
    "    :param target:\n",
    "    :param tokenized_lens:\n",
    "    :param speakers:\n",
    "    :param header_len:\n",
    "    :param s_ids:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "        \n",
    "    cur_idx = header_len\n",
    "    tgt_len = target.shape[0]\n",
    "    for tokenized_len, speaker, s_id in zip(tokenized_lens, speakers, s_ids):\n",
    "        if cur_idx >= tgt_len:\n",
    "            break\n",
    "        elif cur_idx + tokenized_len < tgt_len:\n",
    "            pass\n",
    "            # Check whether the mask is applied to the correct position\n",
    "            # if not torch.equal(target[cur_idx + 2:cur_idx + tokenized_len],\n",
    "            #                    s_id[2:]):\n",
    "            #     logging.warning(\"a sentence mismatches the corresponding piece \"\n",
    "            #                     \"in the conversation\")\n",
    "            #     logging.warning(\"Part 1\")\n",
    "            #     logging.warning(tokenizer.decode(target[cur_idx + 0:cur_idx + tokenized_len]))\n",
    "            #     logging.warning(\"Part 2\")\n",
    "            #     logging.warning(tokenizer.decode( s_id[1:]))\n",
    "        if speaker == \"human\":\n",
    "            # logging.warning(\"Masked:\")\n",
    "            # logging.warning(tokenizer.decode(target[cur_idx:cur_idx + tokenized_len]))\n",
    "            target[cur_idx:cur_idx + tokenized_len] = IGNORE_INDEX\n",
    "        cur_idx += tokenized_len\n",
    "\n",
    "\n",
    "def preprocess(\n",
    "    sources: Sequence[str],\n",
    "    tokenizer: transformers.PreTrainedTokenizer,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Given a list of sources, each is a conversation list. This transform:\n",
    "    1. Add signal '### ' at the beginning each sentence, with end signal '\\n';\n",
    "    2. Concatenate conversations together;\n",
    "    3. Tokenize the concatenated conversation;\n",
    "    4. Make a deepcopy as the target. Mask human words with IGNORE_INDEX.\n",
    "\n",
    "    Note from Shilong: The people at Vicuna didn't bother to update their training code\n",
    "    on GitHub with their newest model, which follows the format in llama_vicuna.ipynb generate\n",
    "    function. I have taken steps to correct it here. In addition, while it make since for them\n",
    "    to use some more sophisticated library to keep track of the prompting format, we don't need\n",
    "    to go to that complication at this point yet. Thus, I removed the conversation lib from the\n",
    "    code, and filled in the appropriate format per the generate function.\n",
    "    \"\"\"\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    header = system\n",
    "    for source in sources:\n",
    "        conversation = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "    # tokenize conversations\n",
    "    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "    input_ids = conversations_tokenized[\"input_ids\"]\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "    header_len = _tokenize_fn([header], tokenizer)[\"input_ids_lens\"][0] - 1\n",
    "    for target, source in zip(targets, sources):\n",
    "        tokenized_sentence = _tokenize_fn([s[\"value\"] for s in source], tokenizer)\n",
    "        tokenized_lens = tokenized_sentence[\"input_ids_lens\"]\n",
    "\n",
    "        # Note from Shilong: TODO: Check to make sure the lengths here is correct, because\n",
    "        # the v0 model's tokenization had to deal with \"###\" but the v1.1 model doesn't\n",
    "        tokenized_lens = [l+1 for l in tokenized_lens]\n",
    "        speakers = [sentence[\"from\"] for sentence in source]\n",
    "        ids = tokenized_sentence[\"input_ids\"]\n",
    "        _mask_targets(target, tokenized_lens, speakers, header_len, ids)\n",
    "\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\n",
    "       Note from Shilong: That means it extended the huggingface dataset. They use huggingface trainer which takes the huggingface\n",
    "       dataset naturally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str,\n",
    "                 tokenizer: transformers.PreTrainedTokenizer):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        # Note from Shilong: See https://github.com/lm-sys/FastChat/blob/main/playground/data/dummy.json for actual example data format, which is in json\n",
    "        list_data_dict = json.load(open(data_path, \"r\"))\n",
    "\n",
    "        logging.warning(\"Formatting inputs...\")\n",
    "        # Note from Shilong: Again, see the https://github.com/lm-sys/FastChat/blob/main/playground/data/dummy.json. It looks like the conversation dict holds the actual data.\n",
    "        sources = [example[\"conversations\"] for example in list_data_dict]\n",
    "\n",
    "\n",
    "        data_dict = preprocess(sources, tokenizer)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Note from Shilong: Those two class/function are pretty much implementation of standard HuggingFace functionality\n",
    "# that can be found via Google Search, i.e. what does datacollator do etc\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    \"\"\"Collate examples for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances]\n",
    "                                  for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels,\n",
    "                                                 batch_first=True,\n",
    "                                                 padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n",
    "\n",
    "\n",
    "def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,\n",
    "                                data_path) -> Dict:\n",
    "    \"\"\"Make dataset and collator for supervised fine-tuning.\"\"\"\n",
    "    dataset_cls = SupervisedDataset\n",
    "    train_dataset = dataset_cls(tokenizer=tokenizer,\n",
    "                                data_path=data_path)\n",
    "    data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "    return dict(train_dataset=train_dataset,\n",
    "                eval_dataset=None,\n",
    "                data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### from transformers import TrainingArguments\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer,\n",
    "                                   output_dir: str,\n",
    "                                  per_device_train_batch_size: int = 8, \n",
    "                                  per_device_eval_batch_size: int = 8):\n",
    "    \"\"\"Collects the state dict and dump to disk.\n",
    "       Note from Shilong: I am not sure why they had this special function. Thus,\n",
    "       I didn't want to touch it.\n",
    "    \"\"\"\n",
    "    state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save:\n",
    "        cpu_state_dict = {\n",
    "            key: value.cpu()\n",
    "            for key, value in state_dict.items()\n",
    "        }\n",
    "        del state_dict\n",
    "        trainer._save(output_dir, state_dict=cpu_state_dict)  # noqa\n",
    "\n",
    "\n",
    "def train(\n",
    "    batch_size: int = 128,\n",
    "    micro_batch_size: int = 4,\n",
    "    num_epochs: int = 3,\n",
    "    learning_rate: float = 3e-4,\n",
    "    cutoff_len: int = 256,\n",
    "    val_set_size: int = 2000,\n",
    "    # lora hyperparams\n",
    "    lora_r: int = 8,\n",
    "    lora_alpha: int = 16,\n",
    "    lora_dropout: float = 0.05,\n",
    "    lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ],\n",
    "    # llm hyperparams\n",
    "    train_on_inputs: bool = True,  # if False, masks out inputs in loss\n",
    "    add_eos_token: bool = False,\n",
    "    group_by_length: bool = False,  # faster, but produces an odd training loss curve\n",
    "):\n",
    "    \n",
    "#    model = transformers.LlamaForCausalLM.from_pretrained(\"/home/jupyter/vicuna-7b\", low_cpu_mem_usage=True, \n",
    "#                                             torch_dtype=torch.float16,\n",
    "#                                             device_map=\"auto\"\n",
    "#                                            )\n",
    "    device_map = \"auto\"\n",
    "    world_size = int(os.environ.get(\"WORLD_SIZE\", 1))\n",
    "    ddp = world_size != 1\n",
    "    if ddp:\n",
    "        device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "        gradient_accumulation_steps = gradient_accumulation_steps // world_size\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # training_args = TrainingArguments(\n",
    "    #        output_dir = output_dir,\n",
    "    #        fp16=True,\n",
    "    #        per_device_train_batch_size=1,\n",
    "    #        per_device_eval_batch_size=1,\n",
    "    #        per_gpu_eval_batch_size=1,\n",
    "    #        per_gpu_train_batch_size=1,\n",
    "    #        report_to = 'none'\n",
    "    # )\n",
    "    \n",
    "    \n",
    "    model = LlamaForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        load_in_8bit=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "#   tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "\n",
    "     \n",
    "#     if tokenizer.pad_token is None:\n",
    "#         smart_tokenizer_and_embedding_resize(\n",
    "#             special_tokens_dict=dict(pad_token=DEFAULT_PAD_TOKEN),\n",
    "#             tokenizer=tokenizer,\n",
    "#             model=model,\n",
    "#         )\n",
    "\n",
    "#     # Note From Shilong: Looks like the end of sequence token is used for beginning of sequence as well as unknown token\n",
    "#     # It may be important to find out if it needs to be done for StableLM or Koala\n",
    "#     if \"llama\" in model_path:\n",
    "#         tokenizer.add_special_tokens({\n",
    "#             \"eos_token\": DEFAULT_EOS_TOKEN,\n",
    "#             \"bos_token\": DEFAULT_BOS_TOKEN,\n",
    "#             \"unk_token\": DEFAULT_UNK_TOKEN,\n",
    "#         })\n",
    "\n",
    "    data_module = make_supervised_data_module(tokenizer=tokenizer,\n",
    "                                              data_path=data_path) \n",
    "    \n",
    "    model = prepare_model_for_int8_training(model)\n",
    "\n",
    "    config = LoraConfig(\n",
    "        r=lora_r,\n",
    "        lora_alpha=lora_alpha,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=lora_dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    model = get_peft_model(model, config)\n",
    "    \n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    gradient_accumulation_steps = batch_size // micro_batch_size\n",
    "    \n",
    "    if not ddp and torch.cuda.device_count() > 1:\n",
    "        # keeps Trainer from trying its own DataParallelism when more than 1 gpu is available\n",
    "        model.is_parallelizable = True\n",
    "        model.model_parallel = True    \n",
    "    \n",
    "    trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        **data_module,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=output_dir,\n",
    "            save_total_limit=3,\n",
    "            load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=group_by_length,\n",
    "            report_to=\"none\",\n",
    "        ),\n",
    "\n",
    "    )\n",
    "    model.config.use_cache = False\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#     model = transformers.LlamaForCausalLM.from_pretrained(\n",
    "#         model_path,\n",
    "#         cache_dir=cache_dir,\n",
    "#         torch_dtype=torch.float16,\n",
    "#         device_map=\"auto\",\n",
    "#         load_in_8bit = True,\n",
    "#     )\n",
    "\n",
    "#     tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "#         model_path,\n",
    "#         cache_dir=cache_dir,\n",
    "#         model_max_length=max_seq_length,\n",
    "#         padding_side=\"right\",\n",
    "#         use_fast=False,\n",
    "#     )\n",
    "\n",
    "    # Note From Shilong: A pad token seems to be artificially added to the model/tokenizer if it does not exists already.\n",
    "    # It may be important to find out if StableLM or Koala need such adjustment.\n",
    "\n",
    "\n",
    "    # Note from Shilong: the **data_module just expand the dictionary from the make_supervised_data_module into arguments\n",
    "    # I removed the args=training_args for now for simplicity, but might need to tune those as well in future.\n",
    "    # See original implementation at: https://github.com/lm-sys/FastChat/blob/dc69abce16fcac6a1d7dab8a7b60cc06f9cf1bb2/fastchat/train/train.py#L281\n",
    "    # and also see the note in the markdown cell\n",
    "#     trainer = Trainer(model=model,\n",
    "#                     tokenizer=tokenizer,\n",
    "#                     args=training_args,\n",
    "#                     **data_module,\n",
    "                      \n",
    "#                      )\n",
    "\n",
    "    if list(pathlib.Path(output_dir).glob(\"checkpoint-*\")):\n",
    "        trainer.train(resume_from_checkpoint=True)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    trainer.save_state()\n",
    "    safe_save_model_for_hf_trainer(trainer=trainer,\n",
    "                                   output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82118da8a234407695a0d094e71785ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Formatting inputs...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how Vicuna is tuned. All of the --parameters gets passed to the train function. I have simplified it down by\n",
    "removing most of the arguments. However, it may be worth seeing the original implementation at\n",
    "https://github.com/lm-sys/FastChat/blob/dc69abce16fcac6a1d7dab8a7b60cc06f9cf1bb2/fastchat/train/train.py#L281\n",
    "to add the parameters back in later. Most importantly, it might be necessary to change the batch size to not\n",
    "run out of memory.\n",
    "\n",
    "torchrun --nproc_per_node=4 --master_port=20001 fastchat/train/train_mem.py \\\n",
    "    --model_name_or_path ~/model_weights/llama-7b  \\\n",
    "    --data_path playground/data/dummy.json \\\n",
    "    --bf16 True \\\n",
    "    --output_dir output \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --evaluation_strategy \"no\" \\\n",
    "    --save_strategy \"steps\" \\\n",
    "    --save_steps 1200 \\\n",
    "    --save_total_limit 10 \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --weight_decay 0. \\\n",
    "    --warmup_ratio 0.03 \\\n",
    "    --lr_scheduler_type \"cosine\" \\\n",
    "    --logging_steps 1 \\\n",
    "    --fsdp \"full_shard auto_wrap\" \\\n",
    "    --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer' \\\n",
    "    --tf32 True \\\n",
    "    --model_max_length 2048 \\\n",
    "    --gradient_checkpointing True \\\n",
    "    --lazy_preprocess True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
