{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7fe91ed-2ba2-440c-9dcf-f58341836ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ecd7673-6a89-4a60-9bb9-fc4217efae8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>published</th>\n",
       "      <th>body</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_type</th>\n",
       "      <th>topic</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reuters</td>\n",
       "      <td>29667</td>\n",
       "      <td>Europe</td>\n",
       "      <td>European shares rise on upbeat cues from Powel...</td>\n",
       "      <td>2023-02-08T17:11:00</td>\n",
       "      <td>Feb 8 (Reuters) - European shares rose on Wedn...</td>\n",
       "      <td>* \\n* STOXX 600 pulls back from 9-mth high to ...</td>\n",
       "      <td>BULLETS</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reuters</td>\n",
       "      <td>79619</td>\n",
       "      <td>China</td>\n",
       "      <td>China Evergrande debt restructuring incentive ...</td>\n",
       "      <td>2023-04-27T06:05:00</td>\n",
       "      <td>HONG KONG, April 27 (Reuters) - Embattled prop...</td>\n",
       "      <td></td>\n",
       "      <td>NULL</td>\n",
       "      <td>310</td>\n",
       "      <td>0.281727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reuters</td>\n",
       "      <td>48010</td>\n",
       "      <td>European Markets</td>\n",
       "      <td>European stocks rally as banking worries fade</td>\n",
       "      <td>2023-03-30T16:27:00</td>\n",
       "      <td>March 30 (Reuters) - European stocks rose to n...</td>\n",
       "      <td>* \\n* H&amp;M posts surprise profit in Dec-Feb\\n* ...</td>\n",
       "      <td>BULLETS</td>\n",
       "      <td>77</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reuters</td>\n",
       "      <td>67859</td>\n",
       "      <td>Aerospace &amp; Defense</td>\n",
       "      <td>China gears up to compete with SpaceX's Starli...</td>\n",
       "      <td>2023-03-02T10:29:00</td>\n",
       "      <td>BEIJING, March 2 (Reuters) - China's military-...</td>\n",
       "      <td></td>\n",
       "      <td>NULL</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reuters</td>\n",
       "      <td>109054</td>\n",
       "      <td>World</td>\n",
       "      <td>Russia's war on Ukraine latest: Moscow denies ...</td>\n",
       "      <td>2023-05-12T05:13:00</td>\n",
       "      <td>May 11 (Reuters) - Russia's defence ministry o...</td>\n",
       "      <td></td>\n",
       "      <td>NULL</td>\n",
       "      <td>719</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source      id             category   \n",
       "0  reuters   29667               Europe  \\\n",
       "1  reuters   79619                China   \n",
       "2  reuters   48010     European Markets   \n",
       "3  reuters   67859  Aerospace & Defense   \n",
       "4  reuters  109054                World   \n",
       "\n",
       "                                               title            published   \n",
       "0  European shares rise on upbeat cues from Powel...  2023-02-08T17:11:00  \\\n",
       "1  China Evergrande debt restructuring incentive ...  2023-04-27T06:05:00   \n",
       "2      European stocks rally as banking worries fade  2023-03-30T16:27:00   \n",
       "3  China gears up to compete with SpaceX's Starli...  2023-03-02T10:29:00   \n",
       "4  Russia's war on Ukraine latest: Moscow denies ...  2023-05-12T05:13:00   \n",
       "\n",
       "                                                body   \n",
       "0  Feb 8 (Reuters) - European shares rose on Wedn...  \\\n",
       "1  HONG KONG, April 27 (Reuters) - Embattled prop...   \n",
       "2  March 30 (Reuters) - European stocks rose to n...   \n",
       "3  BEIJING, March 2 (Reuters) - China's military-...   \n",
       "4  May 11 (Reuters) - Russia's defence ministry o...   \n",
       "\n",
       "                                             summary summary_type  topic   \n",
       "0  * \\n* STOXX 600 pulls back from 9-mth high to ...      BULLETS     -1  \\\n",
       "1                                                            NULL    310   \n",
       "2  * \\n* H&M posts surprise profit in Dec-Feb\\n* ...      BULLETS     77   \n",
       "3                                                            NULL     -1   \n",
       "4                                                            NULL    719   \n",
       "\n",
       "   probability  \n",
       "0     0.000000  \n",
       "1     0.281727  \n",
       "2     1.000000  \n",
       "3     0.000000  \n",
       "4     1.000000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "random.seed(93)\n",
    "np.random.seed(93)\n",
    "\n",
    "\n",
    "topic_df = pd.read_parquet(\"gs://scraped-news-article-data-null/2023-topics.parquet\")\n",
    "topic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cde5bb-838b-4013-ae79-2887663e5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "BASE_PROMPT = \"\"\"A list of news article titles with the published time is given below. Using only the provided information, summarize the theme of the titles such that it will be easy to answer investing related questions from the summary. Be specific about the dates and entities involved and try to not omit important details. Do not use vague terms such as \"past few month\" or \"various companies\".\\n\\n\"\"\"\n",
    "\n",
    "\n",
    "def select_titles(topic_df, topic, limiter, base_prompt=BASE_PROMPT):\n",
    "    topic_segment = topic_df.loc[topic_df.topic == topic][[\"title\", \"published\", \"probability\"]].sort_values(by=\"probability\", ascending=False)\n",
    "    if len(topic_segment.index) == 0:\n",
    "        raise KeyError(\"Invalid Topic: \" + str(topic))\n",
    "    \n",
    "    current_idx = 0\n",
    "    end_idx = len(topic_segment.index)\n",
    "    prompt = base_prompt\n",
    "    row_format = \"Published at: %s Title: %s\\n\"\n",
    "    while limiter(prompt) and current_idx < end_idx:\n",
    "        timestamp = datetime.fromisoformat(topic_segment.iloc[current_idx][\"published\"])\n",
    "        timestamp_str = timestamp.strftime(\"%m/%d/%Y\")\n",
    "        prompt = prompt + row_format % (timestamp_str, topic_segment.iloc[current_idx][\"title\"])\n",
    "        current_idx = current_idx + 1\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_koala_topic_summarizer(prompt=BASE_PROMPT, temperature=0.7, max_new_tokens=512, max_prompt_tokens=1536):\n",
    "    cache_dir = \"/home/jupyter/models\"\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"/home/jupyter/koala_transformer\", device_map=\"auto\", cache_dir=cache_dir,  max_input_size=2048)\n",
    "    model = LlamaForCausalLM.from_pretrained(\"/home/jupyter/koala_transformer\", torch_dtype=torch.float16, low_cpu_mem_usage=True, device_map=\"auto\", cache_dir=cache_dir)\n",
    "    \n",
    "    def generate_koala_prompt(prompt):\n",
    "        system = \"\"\"BEGINNING OF CONVERSATION: \"\"\"\n",
    "        template = system + \"USER: %s GPT:\"\n",
    "        return template % prompt\n",
    "    \n",
    "    def generate_from_tokens(tokens):\n",
    "        outputs = model.generate(**tokens,\n",
    "                             do_sample=True, \n",
    "                             top_p=1.0,\n",
    "                             num_beams=1,\n",
    "                             top_k=50,\n",
    "                             temperature=temperature,\n",
    "                             max_new_tokens=max_new_tokens)\n",
    "        result = tokenizer.decode(outputs[0][tokens[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "        return \"\".join(result)\n",
    "    \n",
    "    def token_count_limiter(final_prompt):\n",
    "        final_prompt = generate_koala_prompt(final_prompt)\n",
    "        tokens = tokenizer(final_prompt, return_tensors=\"pt\")\n",
    "        token_count = tokens[\"input_ids\"].shape[1]\n",
    "        return token_count <= max_prompt_tokens\n",
    "    \n",
    "\n",
    "    def summarize_topics(topic_df, topic_number):\n",
    "        final_prompt = select_titles(topic_df, topic_number, token_count_limiter, base_prompt=prompt)\n",
    "        final_prompt = generate_koala_prompt(final_prompt)\n",
    "        tokens = tokenizer(final_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        return generate_from_tokens(tokens)\n",
    "    \n",
    "    return summarize_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc66cc70-906f-4749-b65a-a8040516293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import re\n",
    "\n",
    "\n",
    "API_PROMPT = \"A list of news article titles with the published time is given below. \" +\\\n",
    "\"Using only the provided information, summarize the theme of the titles such that it will be easy to answer investing related questions from the summary. \" +\\\n",
    "\"Be specific about the dates and entities involved. \" +\\\n",
    "\"Be concise in writing the summary, but try not to omit important details. \" +\\\n",
    "'Do not use vague terms such as \"past few months\", \"various companies\", or \"the disease\". Use the actual names of the entities such as companies, products, etc if possible. ' +\\\n",
    "'Finally, explain how the summary can be used to answer investing related questions. ' +\\\n",
    "'If a clear theme relevant to investing is not present, write \"NO THEME\" as the summary, and explain the reasons. ' +\\\n",
    "'The format of the output should be: <SUMMARY>the summary</SUMMARY><EXPLAINATION>the explanation</EXPLAINATION>\\n\\n'\n",
    "\n",
    "def create_palm2_summarizer(prompt=API_PROMPT, temperature=0.7, max_new_tokens=1024, max_prompt_tokens=2048):\n",
    "    cache_dir = \"/home/jupyter/models\"\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(\"/home/jupyter/koala_transformer\", device_map=\"auto\", cache_dir=cache_dir,  max_input_size=6656)\n",
    "    project_id = \"msca310019-capstone-f945\"\n",
    "    model_name = \"text-bison@001\"\n",
    "    location = \"us-central1\"\n",
    "    matcher = re.compile(r\"\\<SUMMARY\\>(?P<summary>.+)\\<\\/SUMMARY\\>\")\n",
    "    \n",
    "    \"\"\"Predict using a Large Language Model.\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "\n",
    "    def predict_large_language_model_sample(\n",
    "        temperature: float,\n",
    "        max_decode_steps: int,\n",
    "        top_p: float,\n",
    "        top_k: int,\n",
    "        content: str,\n",
    "        ):\n",
    "        \n",
    "        response = model.predict(\n",
    "            content,\n",
    "            temperature=temperature,\n",
    "            max_output_tokens=max_decode_steps,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p)\n",
    "        return response\n",
    "    \n",
    "    def summarize_topics(topic_df, topic_number):\n",
    "        \n",
    "        current_max_token = max_prompt_tokens\n",
    "        \n",
    "        while True:\n",
    "            def token_count_limiter(prompt):\n",
    "                tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
    "                token_count = tokens[\"input_ids\"].shape[1]\n",
    "                return token_count <= current_max_token\n",
    "            final_prompt = select_titles(topic_df, topic_number, token_count_limiter, base_prompt=prompt)\n",
    "            result = predict_large_language_model_sample(temperature=temperature, max_decode_steps=max_new_tokens, top_p=0.8, top_k=40, content=final_prompt)\n",
    "            result = str(result)\n",
    "            match = matcher.search(result)\n",
    "            if match:\n",
    "                return match.group(\"summary\")\n",
    "            else:\n",
    "                current_max_token = current_max_token - 16\n",
    "    \n",
    "    return summarize_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96158b8c-0dd1-4bfc-bf9f-2991d4788751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "with open(\"/home/jupyter/apikey\", \"r\") as api_fp:\n",
    "    api_key = api_fp.read().strip()\n",
    "\n",
    "    \n",
    "OPENAI_PROMPT = \"A list of news article titles with the published time is given below. \" +\\\n",
    "\"Using only the provided information, summarize the theme of the titles such that it will be easy to answer investing related questions from the summary. \" +\\\n",
    "\"Be specific about the dates and entities involved. \" +\\\n",
    "\"Be concise in writing the summary, but try not to omit important details. \" +\\\n",
    "'Do not use vague terms such as \"past few months\", \"various companies\", or \"the disease\". Use the actual names if possible. ' +\\\n",
    "'Finally, explain how the summary can be used to answer investing related questions. ' +\\\n",
    "'If a clear theme relevant to investing is not present, maintain the specified format, use \"SUMMARY: NO THEME\" as the summary, and explain the reasons. ' +\\\n",
    "'The format of the output should be:\\nSUMMARY:\\nthe summary\\nEXPLANATION:\\nthe explanation\\n\\n'\n",
    "    \n",
    "\n",
    "def create_openai_summarizer(api_key, prompt=OPENAI_PROMPT, temperature=0.7, max_new_tokens=1024, max_prompt_tokens=3000):\n",
    "    openai.api_key = api_key\n",
    "    engine = \"gpt-3.5-turbo\"\n",
    "    encoding = tiktoken.encoding_for_model(engine)\n",
    "    matcher = re.compile(r\"SUMMARY:(?P<summary>(.|\\n)+)EXPLANATION\")\n",
    "    \n",
    "    @retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "    def completion_with_backoff(**kwargs):\n",
    "        return openai.ChatCompletion.create(**kwargs)\n",
    "    \n",
    "    def token_count_limiter(prompt):\n",
    "        tokens = encoding.encode(prompt)\n",
    "        return len(tokens) <= max_prompt_tokens\n",
    "    \n",
    "    def summarize_topic(topic_df, topic_number):\n",
    "        final_prompt = select_titles(topic_df, topic_number, token_count_limiter, base_prompt=prompt)\n",
    "        messages=[\n",
    "            {'role': 'user', 'content': final_prompt},\n",
    "        ]\n",
    "        result = completion_with_backoff(messages=messages, model=engine, temperature=temperature, max_tokens=max_new_tokens)\n",
    "        result_text = result['choices'][0]['message']['content']\n",
    "        match = matcher.search(result_text)\n",
    "        if match:\n",
    "            return match.group(\"summary\").strip()\n",
    "        else:\n",
    "            raise AssertionError(\"Did not generate correct response:\\n\" + result_text)\n",
    "            \n",
    "    return summarize_topic\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b437ae12-98fa-4c3f-972b-1d0084e5fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_topic_summarizer(kind=\"vertex-ai\", **kwargs):\n",
    "    if kind == \"koala\":\n",
    "        return create_koala_topic_summarizer(temperature=0.01)\n",
    "    if kind == \"vertex-ai\":\n",
    "        return create_palm2_summarizer(temperature=0)\n",
    "    if kind == \"openai\":\n",
    "        return create_openai_summarizer(api_key, temperature=0)\n",
    "    raise ValueError(\"Invalid kind: \" + kind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c94fc2c-4e41-4465-898c-ba633a141ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft and Google are investing heavily in A.I., and there are many potential applications for A.I. in the workplace. Investors should keep an eye on these developments as they could have a significant impact on the future of work.\n"
     ]
    }
   ],
   "source": [
    "api_summarizer = create_topic_summarizer()\n",
    "test = api_summarizer(topic_df, 0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27d9dc7b-bed7-4d5c-9a22-fb2679368e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The articles discuss various developments and applications of artificial intelligence (AI) technology, particularly in the form of generative language models like ChatGPT and Bard. Companies like Google, Microsoft, and OpenAI are investing heavily in AI and developing new products and features that incorporate the technology. AI is being used in a variety of industries, including finance, real estate, and healthcare, and is seen as a potential tool for improving productivity and employee engagement. However, there are also concerns about the risks and ethical implications of AI, and regulators are beginning to investigate the technology.\n"
     ]
    }
   ],
   "source": [
    "openai_summarizer = create_topic_summarizer(kind=\"openai\")\n",
    "test = openai_summarizer(topic_df, 0)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00309f1-8bf4-4ff3-8b8c-a8fbee030370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1564\n"
     ]
    }
   ],
   "source": [
    "topics = list(set(topic_df.topic.unique()) - {-1})\n",
    "print(len(topics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01560cb4-6420-46a5-b6ac-4d89668ec6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0308818a-aac4-44a2-91be-e988ddb517ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>The news articles revolve around the advanceme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The news articles cover the fluctuations in oi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The news articles revolve around the security ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The Federal Reserve's interest rate decisions ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The news articles cover various aspects of the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topics                                            summary\n",
       "0       0  The news articles revolve around the advanceme...\n",
       "1       1  The news articles cover the fluctuations in oi...\n",
       "2       2  The news articles revolve around the security ...\n",
       "3       3  The Federal Reserve's interest rate decisions ...\n",
       "4       4  The news articles cover various aspects of the..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer_type = \"openai\"\n",
    "\n",
    "\n",
    "try:\n",
    "    topic_sum = pd.read_parquet(\"gs://scraped-news-article-data-null/2023-topics-%s.parquet\" % summarizer_type)\n",
    "except:\n",
    "    topic_sum = pd.DataFrame({\n",
    "        \"topics\": topics,\n",
    "        \"summary\": [\"\" for _ in topics]\n",
    "    })\n",
    "\n",
    "topic_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d809d2c9-2e1f-42c0-8e8f-1d2061cd9143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 251/1498 [1:22:20<6:49:05, 19.68s/it]\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "works = set(topic_sum.loc[topic_sum.summary.str.len() == 0][\"topics\"].to_list())\n",
    "summarizer = create_topic_summarizer(summarizer_type)\n",
    "with tqdm(total = len(works)) as progress:\n",
    "    for i, w in enumerate(works):\n",
    "        topic = topic_sum.loc[topic_sum.topics == w].iloc[0][\"topics\"]\n",
    "        summary = summarizer(topic_df, topic)\n",
    "        topic_sum.loc[topic_sum.topics == w, \"summary\"] = summary\n",
    "        progress.update(1)\n",
    "        if i % 10 == 0:\n",
    "            topic_sum.to_parquet(\"gs://scraped-news-article-data-null/2023-topics-%s.parquet\" % summarizer_type, index=False)\n",
    "topic_sum.to_parquet(\"gs://scraped-news-article-data-null/2023-topics-%s.parquet\" % summarizer_type, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b314d3f-2068-474a-bb33-d5944777cf6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "llamaidx",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "llamaidx",
   "language": "python",
   "name": "llamaidx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
