{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#%pip install allennlp\n",
    "#%pip install --pre allennlp-models\n",
    "#%pip install google-cloud-storage\n",
    "#%pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from typing import Dict, List\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import Span\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pandarallel import pandarallel\n",
    "from contextlib import closing\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_coref_index(client, bucket=\"meta-info\", coref_index=\"coref-index.json\"):\n",
    "    files = set([f.name for f in client.list_blobs(bucket_or_name=bucket)])\n",
    "    if coref_index not in files:\n",
    "        return {\n",
    "            \"error\": []\n",
    "        }\n",
    "    bucket = client.bucket(bucket)\n",
    "    with bucket.blob(coref_index).open(\"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "        if \"error\" not in index:\n",
    "            index[\"error\"] = []\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_coref_converted(client, bucket=\"markdown-corref\"):\n",
    "    files = set([f.name for f in client.list_blobs(bucket_or_name=bucket)])\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_coref_work(source=\"markdown-converged\", filter_f=lambda article: True):\n",
    "    with closing(storage.Client(project=\"msca310019-capstone-f945\")) as client:\n",
    "        errors = set(load_coref_index(client)[\"error\"])\n",
    "        done = get_coref_converted(client)\n",
    "        result = []\n",
    "        bucket = client.bucket(source)\n",
    "        tbd = set([f.name for f in client.list_blobs(bucket_or_name=source)])\n",
    "        tbd = tbd - done - errors\n",
    "        tbd_df = pd.DataFrame({\n",
    "            \"tbd\": list(tbd)\n",
    "        })\n",
    "    \n",
    "    def filter_article(f_name):\n",
    "        with closing(storage.Client(project=\"msca310019-capstone-f945\")) as client:\n",
    "            bucket = client.bucket(source)\n",
    "            with bucket.blob(f_name).open(\"r\") as fp:\n",
    "                article = json.load(fp)\n",
    "                if filter_f(article):\n",
    "                    return [f_name, article]\n",
    "        return []\n",
    "    \n",
    "    tbd_df[\"result\"] = tbd_df.tbd.parallel_apply(filter_article)\n",
    "    tbd_df_filtered = tbd_df.loc[tbd_df[\"result\"].str.len() > 0]\n",
    "    \n",
    "    return tbd_df_filtered.result.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "253f541935a54cb098bea887c64abcaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=4565), Label(value='0 / 4565'))), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9374"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def year_filter(article, year=2023):\n",
    "    timestamp = datetime.fromisoformat(article[\"published\"])\n",
    "    return timestamp.year == year\n",
    "\n",
    "client = storage.Client(project=\"msca310019-capstone-f945\")\n",
    "works = get_coref_work(filter_f=year_filter)\n",
    "\n",
    "len(works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]):\n",
    "        spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "        spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "        span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "            if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "        return span_noun_indices\n",
    "\n",
    "\n",
    "def get_cluster_head_idx(doc, cluster):\n",
    "    noun_indices = get_span_noun_indices(doc, cluster)\n",
    "    return noun_indices[0] if noun_indices else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def print_clusters(doc, clusters):\n",
    "    def get_span_words(span, allen_document):\n",
    "        return ' '.join(allen_document[span[0]:span[1]+1])\n",
    "\n",
    "    allen_document, clusters = [t.text for t in doc], clusters\n",
    "    for cluster in clusters:\n",
    "        cluster_head_idx = get_cluster_head_idx(doc, cluster)\n",
    "        if cluster_head_idx >= 0:\n",
    "            cluster_head = cluster[cluster_head_idx]\n",
    "            print(get_span_words(cluster_head, allen_document) + ' - ', end='')\n",
    "            print('[', end='')\n",
    "            for i, span in enumerate(cluster):\n",
    "                print(get_span_words(span, allen_document) + (\"; \" if i+1 < len(cluster) else \"\"), end='')\n",
    "            print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def original_replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
    "        mention_span = document[mention_start:mention_end]\n",
    "\n",
    "        for coref in cluster[1:]:\n",
    "            core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "\n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "allen_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\"\n",
    "gpu_predictor = Predictor.from_path(allen_url, cuda_device=torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "spacy.require_cpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def fetch_articles(client, max_id, bucket=\"markdown-converged\"):\n",
    "    bucket = client.bucket(bucket)\n",
    "    for i in range(coref_index[\"standardized\"], max_id):\n",
    "        file_name = \"%s.json\" % i\n",
    "        with bucket.blob(file_name).open(\"r\") as fp:\n",
    "            try:\n",
    "                article_dict = json.load(fp)\n",
    "                yield i, article_dict\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def window_sentences(sentences, idx, pre=5, sep = \"\\n\\n\"):\n",
    "    start_idx = max(0, idx - pre)\n",
    "    context = \" \".join(sentences[start_idx:idx])\n",
    "    context = re.sub(r\"\\s+\", \" \", context)\n",
    "    result = context + \" \" + sep + \" \" + sentences[idx]\n",
    "    return result\n",
    "\n",
    "\n",
    "def coref_text_whole(article, predictor):\n",
    "    article = article.strip()\n",
    "    if len(article) == 0:\n",
    "        return \"\"\n",
    "    clusters = predictor.predict(article)['clusters']\n",
    "    doc = nlp(article)\n",
    "    coref_article = improved_replace_corefs(doc, clusters)\n",
    "    return coref_article\n",
    "\n",
    "\n",
    "def coref_text_parts(sentences, predictor):\n",
    "    sentences = list(sentences)\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        shard = window_sentences(sentences, i)\n",
    "        clusters = predictor.predict(shard)['clusters']\n",
    "        doc = nlp(shard)\n",
    "        coref_shard = improved_replace_corefs(doc, clusters)\n",
    "        replacement_parts = coref_shard.split(\"\\n\\n\")\n",
    "        if len(replacement_parts) > 2:\n",
    "            raise ValueError(\"Incorrect number of parts: \" + str(len(replacement_parts)))\n",
    "        replacement = replacement_parts[1].strip()\n",
    "        sentences[i] = replacement\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def coref_text(article):\n",
    "    try:\n",
    "        return coref_text_whole(article, gpu_predictor)\n",
    "    except Exception:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/9374 [00:05<2:26:57,  1.06it/s] /opt/conda/lib/python3.7/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9374/9374 [1:30:22<00:00,  1.73it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2376/460730249.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprogress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mcoref_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"standardized\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "bucket = client.bucket(\"markdown-corref\")\n",
    "idx = load_coref_index(client)\n",
    "errors = set(idx[\"error\"])\n",
    "\n",
    "\n",
    "with tqdm(total=len(works)) as progress:\n",
    "    for f_name, article in works:\n",
    "        corref_body = \"\"\n",
    "        if len(article[\"body\"]) > 0:\n",
    "            corref_body = coref_text(article[\"body\"])\n",
    "        if corref_body:\n",
    "            with bucket.blob(f_name).open(\"w\") as fp:\n",
    "                article[\"body\"] = corref_body\n",
    "                json.dump(fp=fp, obj=article)\n",
    "        else:\n",
    "            errors.add(f_name)\n",
    "        progress.update(1)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[\"error\"] = list(errors)\n",
    "def write_conversion_index(client, index, bucket=\"meta-info\", conversion_index=\"coref-index.json\"):\n",
    "    bucket = client.bucket(bucket)\n",
    "    with bucket.blob(conversion_index).open(\"w\") as fp:\n",
    "        json.dump(fp=fp, obj=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_conversion_index(client, idx)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
