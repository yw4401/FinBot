{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-13T15:14:59.517630Z",
     "start_time": "2023-09-13T15:14:59.499403Z"
    }
   },
   "outputs": [],
   "source": [
    "#%pip install allennlp\n",
    "#%pip install --pre allennlp-models\n",
    "#%pip install google-cloud-storage\n",
    "#%pip install pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:21:21.454247Z",
     "start_time": "2023-09-13T15:21:15.987332Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\r\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.8/12.8 MB\u001B[0m \u001B[31m22.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from en-core-web-sm==3.3.0) (3.3.3)\r\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.12)\r\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.5)\r\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.9)\r\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\r\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.8)\r\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.17)\r\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.10)\r\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\r\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.7)\r\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.9)\r\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\r\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.2)\r\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (6.4.0)\r\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.66.1)\r\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.25.2)\r\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.31.0)\r\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\r\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.1.2)\r\n",
      "Requirement already satisfied: setuptools in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (68.0.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (23.1)\r\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.7.4.1 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.5.0)\r\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.2.0)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2023.7.22)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.1.7)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.1.3)\r\n",
      "Installing collected packages: en-core-web-sm\r\n",
      "Successfully installed en-core-web-sm-3.3.0\r\n",
      "\u001B[38;5;2m✔ Download and installation successful\u001B[0m\r\n",
      "You can now load the package via spacy.load('en_core_web_sm')\r\n"
     ]
    }
   ],
   "source": [
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:15:03.814686Z",
     "start_time": "2023-09-13T15:15:00.467225Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 3 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "from typing import Dict, List\n",
    "from spacy.tokens import Doc\n",
    "from spacy.tokens import Span\n",
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from pandarallel import pandarallel\n",
    "from contextlib import closing\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "pandarallel.initialize(nb_workers=3, progress_bar=True)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:15:04.594800Z",
     "start_time": "2023-09-13T15:15:04.570145Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_coref_index(client, bucket=\"meta-info\", coref_index=\"coref-index.json\"):\n",
    "    files = set([f.name for f in client.list_blobs(bucket_or_name=bucket)])\n",
    "    if coref_index not in files:\n",
    "        return {\n",
    "            \"error\": []\n",
    "        }\n",
    "    bucket = client.bucket(bucket)\n",
    "    with bucket.blob(coref_index).open(\"r\") as fp:\n",
    "        index = json.load(fp)\n",
    "        if \"error\" not in index:\n",
    "            index[\"error\"] = []\n",
    "    return index\n",
    "\n",
    "\n",
    "def get_coref_converted(client, bucket=\"markdown-corref\"):\n",
    "    files = set([f.name for f in client.list_blobs(bucket_or_name=bucket)])\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_coref_work(source=\"markdown-converged\", filter_f=lambda article: True):\n",
    "    with closing(storage.Client(project=\"msca310019-capstone-f945\")) as client:\n",
    "        errors = set(load_coref_index(client)[\"error\"])\n",
    "        done = get_coref_converted(client)\n",
    "        tbd = set([f.name for f in client.list_blobs(bucket_or_name=source)])\n",
    "        tbd = tbd - done - errors\n",
    "        tbd_df = pd.DataFrame({\n",
    "            \"tbd\": list(tbd)\n",
    "        })\n",
    "    \n",
    "    def filter_article(f_name):\n",
    "        with closing(storage.Client(project=\"msca310019-capstone-f945\")) as client:\n",
    "            bucket = client.bucket(source)\n",
    "            with bucket.blob(f_name).open(\"r\") as fp:\n",
    "                article = json.load(fp)\n",
    "                if filter_f(article):\n",
    "                    return [f_name, article]\n",
    "        return []\n",
    "    \n",
    "    tbd_df[\"result\"] = tbd_df.tbd.parallel_apply(filter_article)\n",
    "    tbd_df_filtered = tbd_df.loc[tbd_df[\"result\"].str.len() > 0]\n",
    "    \n",
    "    return tbd_df_filtered.result.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-13T15:16:48.168676Z",
     "start_time": "2023-09-13T15:15:06.703936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=17430), Label(value='0 / 17430')))…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08cb159bfe894f81af05d78e42fd10db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process ForkPoolWorker-6:\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-10:\n",
      "Process ForkPoolWorker-8:\n",
      "Process ForkPoolWorker-9:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/pool.py\", line 114, in worker\n",
      "    task = get()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 365, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/queues.py\", line 364, in get\n",
      "    with self._rlock:\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/opt/homebrew/anaconda3/envs/FinBot/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "def year_filter(article, year=2023):\n",
    "    timestamp = datetime.fromisoformat(article[\"published\"])\n",
    "    return timestamp.year == year\n",
    "\n",
    "client = storage.Client(project=\"msca310019-capstone-f945\")\n",
    "works = get_coref_work(filter_f=year_filter)\n",
    "\n",
    "len(works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_span_noun_indices(doc: Doc, cluster: List[List[int]]):\n",
    "        spans = [doc[span[0]:span[1]+1] for span in cluster]\n",
    "        spans_pos = [[token.pos_ for token in span] for span in spans]\n",
    "        span_noun_indices = [i for i, span_pos in enumerate(spans_pos)\n",
    "            if any(pos in span_pos for pos in ['NOUN', 'PROPN'])]\n",
    "        return span_noun_indices\n",
    "\n",
    "\n",
    "def get_cluster_head_idx(doc, cluster):\n",
    "    noun_indices = get_span_noun_indices(doc, cluster)\n",
    "    return noun_indices[0] if noun_indices else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_clusters(doc, clusters):\n",
    "    def get_span_words(span, allen_document):\n",
    "        return ' '.join(allen_document[span[0]:span[1]+1])\n",
    "\n",
    "    allen_document, clusters = [t.text for t in doc], clusters\n",
    "    for cluster in clusters:\n",
    "        cluster_head_idx = get_cluster_head_idx(doc, cluster)\n",
    "        if cluster_head_idx >= 0:\n",
    "            cluster_head = cluster[cluster_head_idx]\n",
    "            print(get_span_words(cluster_head, allen_document) + ' - ', end='')\n",
    "            print('[', end='')\n",
    "            for i, span in enumerate(cluster):\n",
    "                print(get_span_words(span, allen_document) + (\"; \" if i+1 < len(cluster) else \"\"), end='')\n",
    "            print(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def core_logic_part(document: Doc, coref: List[int], resolved: List[str], mention_span: Span):\n",
    "    final_token = document[coref[1]]\n",
    "    if final_token.tag_ in [\"PRP$\", \"POS\"]:\n",
    "        resolved[coref[0]] = mention_span.text + \"'s\" + final_token.whitespace_\n",
    "    else:\n",
    "        resolved[coref[0]] = mention_span.text + final_token.whitespace_\n",
    "    for i in range(coref[0] + 1, coref[1] + 1):\n",
    "        resolved[i] = \"\"\n",
    "    return resolved\n",
    "\n",
    "\n",
    "def original_replace_corefs(document: Doc, clusters: List[List[List[int]]]) -> str:\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "\n",
    "    for cluster in clusters:\n",
    "        mention_start, mention_end = cluster[0][0], cluster[0][1] + 1\n",
    "        mention_span = document[mention_start:mention_end]\n",
    "\n",
    "        for coref in cluster[1:]:\n",
    "            core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n",
    "\n",
    "\n",
    "def get_cluster_head(doc: Doc, cluster: List[List[int]], noun_indices: List[int]):\n",
    "    head_idx = noun_indices[0]\n",
    "    head_start, head_end = cluster[head_idx]\n",
    "    head_span = doc[head_start:head_end+1]\n",
    "    return head_span, [head_start, head_end]\n",
    "\n",
    "\n",
    "def is_containing_other_spans(span: List[int], all_spans: List[List[int]]):\n",
    "    return any([s[0] >= span[0] and s[1] <= span[1] and s != span for s in all_spans])\n",
    "\n",
    "\n",
    "def improved_replace_corefs(document, clusters):\n",
    "    resolved = list(tok.text_with_ws for tok in document)\n",
    "    all_spans = [span for cluster in clusters for span in cluster]  # flattened list of all spans\n",
    "\n",
    "    for cluster in clusters:\n",
    "        noun_indices = get_span_noun_indices(document, cluster)\n",
    "\n",
    "        if noun_indices:\n",
    "            mention_span, mention = get_cluster_head(document, cluster, noun_indices)\n",
    "\n",
    "            for coref in cluster:\n",
    "                if coref != mention and not is_containing_other_spans(coref, all_spans):\n",
    "                    core_logic_part(document, coref, resolved, mention_span)\n",
    "\n",
    "    return \"\".join(resolved)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "allen_url = \"https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz\"\n",
    "gpu_predictor = Predictor.from_path(allen_url, cuda_device=torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spacy.require_cpu()\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_articles(client, max_id, bucket=\"markdown-converged\"):\n",
    "    bucket = client.bucket(bucket)\n",
    "    for i in range(coref_index[\"standardized\"], max_id):\n",
    "        file_name = \"%s.json\" % i\n",
    "        with bucket.blob(file_name).open(\"r\") as fp:\n",
    "            try:\n",
    "                article_dict = json.load(fp)\n",
    "                yield i, article_dict\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "def window_sentences(sentences, idx, pre=5, sep = \"\\n\\n\"):\n",
    "    start_idx = max(0, idx - pre)\n",
    "    context = \" \".join(sentences[start_idx:idx])\n",
    "    context = re.sub(r\"\\s+\", \" \", context)\n",
    "    result = context + \" \" + sep + \" \" + sentences[idx]\n",
    "    return result\n",
    "\n",
    "\n",
    "def coref_text_whole(article, predictor):\n",
    "    article = article.strip()\n",
    "    if len(article) == 0:\n",
    "        return \"\"\n",
    "    clusters = predictor.predict(article)['clusters']\n",
    "    doc = nlp(article)\n",
    "    coref_article = improved_replace_corefs(doc, clusters)\n",
    "    return coref_article\n",
    "\n",
    "\n",
    "def coref_text_parts(sentences, predictor):\n",
    "    sentences = list(sentences)\n",
    "\n",
    "    for i in range(len(sentences)):\n",
    "        shard = window_sentences(sentences, i)\n",
    "        clusters = predictor.predict(shard)['clusters']\n",
    "        doc = nlp(shard)\n",
    "        coref_shard = improved_replace_corefs(doc, clusters)\n",
    "        replacement_parts = coref_shard.split(\"\\n\\n\")\n",
    "        if len(replacement_parts) > 2:\n",
    "            raise ValueError(\"Incorrect number of parts: \" + str(len(replacement_parts)))\n",
    "        replacement = replacement_parts[1].strip()\n",
    "        sentences[i] = replacement\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def coref_text(article):\n",
    "    try:\n",
    "        return coref_text_whole(article, gpu_predictor)\n",
    "    except Exception:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/9374 [00:05<2:26:57,  1.06it/s] /opt/conda/lib/python3.7/site-packages/allennlp/modules/token_embedders/pretrained_transformer_embedder.py:385: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  num_effective_segments = (seq_lengths + self._max_length - 1) // self._max_length\n",
      "100%|██████████| 9374/9374 [1:30:22<00:00,  1.73it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/tmp/ipykernel_2376/460730249.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     16\u001B[0m             \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mf_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     17\u001B[0m         \u001B[0mprogress\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 18\u001B[0;31m     \u001B[0mcoref_index\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"standardized\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     19\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "bucket = client.bucket(\"markdown-corref\")\n",
    "idx = load_coref_index(client)\n",
    "errors = set(idx[\"error\"])\n",
    "\n",
    "\n",
    "with tqdm(total=len(works)) as progress:\n",
    "    for f_name, article in works:\n",
    "        corref_body = \"\"\n",
    "        if len(article[\"body\"]) > 0:\n",
    "            corref_body = coref_text(article[\"body\"])\n",
    "        if corref_body:\n",
    "            with bucket.blob(f_name).open(\"w\") as fp:\n",
    "                article[\"body\"] = corref_body\n",
    "                json.dump(fp=fp, obj=article)\n",
    "        else:\n",
    "            errors.add(f_name)\n",
    "        progress.update(1)    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx[\"error\"] = list(errors)\n",
    "def write_conversion_index(client, index, bucket=\"meta-info\", conversion_index=\"coref-index.json\"):\n",
    "    bucket = client.bucket(bucket)\n",
    "    with bucket.blob(conversion_index).open(\"w\") as fp:\n",
    "        json.dump(fp=fp, obj=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_conversion_index(client, idx)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
