{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4346817c-f0a7-4bcf-8ae8-f04694dce277",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407edfef-4f4c-403d-bc13-7eaf31547b29",
   "metadata": {},
   "source": [
    "The parameters\n",
    "\n",
    "```\n",
    "padding_side=\"left\", truncation_side=\"right\", add_bos_token=False\n",
    "```\n",
    "\n",
    "were taken from the EasyLM llama_model.py code. It is uncertain whether it is the correct configuration, but it seems to run fine for now.\n",
    "\n",
    "\n",
    "The parameter\n",
    "\n",
    "```\n",
    "torch_dtype=torch.float16\n",
    "```\n",
    "\n",
    "ensures that 16 bit float precision is used instead of 32 bit, which theoretically reduces the memory requirement to less than 16G of GPU RAM. However, in practice, \n",
    "it turned out that sometimes 16G is not sufficient perhaps due to loading the input on CUDA as well, so move up to 2 T4 or increase total GPU RAM by other means to beyond 16G. Alternatively, perhaps there's a way to clear the GPU RAM before generating to make space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02808b0-f749-4246-ae04-44d0d24c554f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad606c6256a248f6a55bdab131c476d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"/home/jupyter/koala_transformer\", device_map=\"auto\", padding_side=\"left\", truncation_side=\"right\", add_bos_token=False)\n",
    "\n",
    "# full precision - requires slightly more than 32G of GPU RAM in practice\n",
    "# model = LlamaForCausalLM.from_pretrained(\"/home/jupyter/koala_transformer\", device_map=\"auto\", cache_dir=\"/home/jupyter/data/transformers\")\n",
    "\n",
    "# half precision - requires slightly more than 16G of GPU RAM in practice.\n",
    "model = LlamaForCausalLM.from_pretrained(\"/home/jupyter/koala_transformer\", torch_dtype=torch.float16, device_map=\"auto\", cache_dir=\"/home/jupyter/data/transformers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16452463-946e-42df-942a-331bfee17c10",
   "metadata": {},
   "source": [
    "The generate function is adapted to the correct format required by https://github.com/young-geng/EasyLM/blob/main/docs/koala.md. Again the parameters\n",
    "\n",
    "```\n",
    "do_sample=True, \n",
    "top_p=1.0,\n",
    "num_beams=1,\n",
    "top_k=50,\n",
    "```\n",
    "\n",
    "are taken from the EasyLM code.\n",
    "\n",
    "The temperature controls how stochastic or creative the model is, and it's a common parameter to adjust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3f7a6ac-0c37-41b5-9fde-74ea8a86e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt, max_new_tokens=1024, temperature=0.1):\n",
    "    \n",
    "    # Create the prompt according to the pre-trained format: see https://github.com/young-geng/EasyLM/blob/main/docs/koala.md\n",
    "    \n",
    "    template = \"BEGINNING OF CONVERSATION: USER: %s \"\n",
    "    input_start = template % prompt\n",
    "    input_final = template % prompt + \"GPT:\"\n",
    "    \n",
    "    inputs = tokenizer(input_final, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, \n",
    "                             do_sample=True, \n",
    "                             top_p=1.0,\n",
    "                             num_beams=1,\n",
    "                             top_k=50,\n",
    "                             temperature=temperature)\n",
    "    result = tokenizer.batch_decode(outputs, skip_special_tokens=True, spaces_between_special_tokens=False)\n",
    "    \n",
    "    # Since the task is Autoregressive LM, it's gonna repeat the prompt in the output. Thus, the prompt itself is removed from the output.\n",
    "    \n",
    "    return result[0][len(input_start):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b061f7e-2e2f-48eb-99dd-6c7afbd501d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT: A BLT (bacon, lettuce, and tomato) is a classic sandwich that is made with bacon, lettuce, and tomato on a slice of bread. Here's how to make a BLT:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "*   1 slice of bread\n",
      "*   1/2 cup of bacon, cooked according to your preference (e.g. crispy, medium, or soft)\n",
      "*   1/2 cup of lettuce\n",
      "*   1/2 cup of tomato\n",
      "*   Mayo or other desired condiments\n",
      "*   Salt and pepper, to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1.   Toast the bread to your desired level of crispiness.\n",
      "2.   Layer the bacon, lettuce, and tomato on the bread.\n",
      "3.   Spread the mayo or other desired condiments on the bread.\n",
      "4.   Sprinkle salt and pepper on the bread.\n",
      "5.   Serve and enjoy!\n",
      "\n",
      "Note: You can also add other ingredients to the BLT, such as avocado, cucumber, or other vegetables, to make it your own.\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"How to make a BLT?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755c8926-dddf-4399-85bb-c65eb72310f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT: Shrimp tempura is a delicious Japanese dish made by battering and frying shrimp in a light, crispy batter. Here's how to make shrimp tempura at home:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "*   1 cup flour\n",
      "*   1 cup cornstarch\n",
      "*   1 cup water\n",
      "*   1 cup vegetable oil\n",
      "*   1 cup shrimp, peeled and deveined\n",
      "*   Salt and pepper, to taste\n",
      "*   Batter:\n",
      "    \n",
      "    *   1 cup flour\n",
      "    *   1 cup cornstarch\n",
      "    *   1 cup water\n",
      "    *   1 cup vegetable oil\n",
      "    *   1 cup beer or sake\n",
      "    *   1 cup soy sauce\n",
      "    *   1 cup sugar\n",
      "    *   1 cup dashi (a broth made from bonito flakes)\n",
      "    \n",
      "    \n",
      "    \n",
      "\n",
      "Instructions:\n",
      "\n",
      "1.   In a small bowl, combine the flour, cornstarch, water, vegetable oil, beer or sake, soy sauce, and sugar. Mix well to form a smooth batter.\n",
      "2.   In a large pan, heat the vegetable oil over medium heat. Once the oil is hot, add the shrimp and cook for 1-2 minutes, or until the shrimp are cooked through.\n",
      "3.   Remove the shrimp from the pan and drain on a paper towel.\n",
      "4.   In a small bowl, combine the batter and the shrimp. Mix well to coat the shrimp evenly.\n",
      "5.   Heat the vegetable oil in the pan over medium heat. Once the oil is hot, add the coated shrimp and cook for 1-2 minutes, or until the shrimp are cooked through.\n",
      "6.   Remove the shrimp from the pan and drain on a paper towel.\n",
      "7.   Serve the shrimp tempura with a dipping sauce, such as soy sauce, wasabi, or a mixture of both.\n",
      "\n",
      "Note: This recipe can be adjusted to your taste by adding more or less of the ingredients. You can also add other ingredients, such as vegetables or seafood, to the batter to make it more flavorful.\n"
     ]
    }
   ],
   "source": [
    "print(generate(\"How to make shrimp tempura?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd205f5-8e6d-4050-9dba-4fc989e1224f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT: The article discusses the treatment options for Alzheimer's disease and the companies that are developing therapies for this condition. Biogen, Intra-Cellular Therapies, and Axsome Therapeutics are three companies that are currently developing therapies for Alzheimer's disease. Biogen's drug Aduhelm faced pushback in 2022, but the company is planning to give it another go with its new drug LEQEMBI. Intra-Cellular Therapies is currently evaluating ITI-1284-ODT-SL, a deuterated form of lumateperone that comes in an oral disintegrating tablet. Axsome Therapeutics is developing and commercializing a portfolio of novel therapies for disorders of the central nervous system with limited treatment options. Its product pipeline includes AXS-05, a NMDA receptor antagonist with multimodal activity that is currently in Phase 3 clinical studies for the treatment of Alzheimer's disease agitation.\n"
     ]
    }
   ],
   "source": [
    "article = \"\"\"Alzheimer’s disease treatment stocks are focused on Alzheimer’s disease, a degenerative brain disorder that results in declining memory and thinking skills and typically affects people in their mid-60s.According to the Alzheimer’s Association, neurons in other areas of the brain also begin to deteriorate as Alzheimer’s disease gets worse, resulting in the loss of basic human functions and overall cognitive impairment.This condition affects more than 6.7 million people in the US alone; it’s also the most common form of dementia and is the sixth leading cause of death in America. Treatments are available to alleviate Alzheimer’s disease symptoms, but because they do not affect the underlying causes of this neurodegenerative disease, they’re only a bandaid solution.Therapies approved by the US Food and Drug Administration (FDA) include: rivastigmine by Novartis (NYSE:NVS); galantamine, developed by Janssen, a division of Johnson & Johnson (NYSE:JNJ); donepezil by Pfizer (NYSE:PFE); and AbbVie's (NYSE:ABBV) memantine.Since there is no cure for Alzheimer’s disease, death is often the result for patients as the ailment devastates the brain. And unfortunately, Alzheimer’s disease is rising in prevalence — a report from Research and Markets suggests that the global Alzheimer’s disease treatment market will be worth a significant US$9.64 billion by 2028 as more patients need treatment, and as more investments are made in biomarkers for diagnosis and drug development.Here the Investing News Network takes a look at a few of the biggest Alzheimer’s disease treatment stocks on the NASDAQ. Companies are listed in order of market cap from largest to smallest, and all data was current as of April 19, 2023.. Biogen (NASDAQ:BIIB) Company Profile.Market cap: US$42.27 billion; current share price: US$292.80.Massachusetts-based Biogen, a pioneer in the field of neuroscience, is developing, manufacturing and marketing therapies aimed at treating serious neurological, neurodegenerative, autoimmune and rare diseases.The global biotechnology firm’s research areas include Alzheimer's disease and dementia. However, Biogen’s launch of its FDA-approved Alzheimer’s disease drug Aduhelm faced a lot of pushback in 2022, both from the market and from Congress, over what was viewed as a hasty fast-track approval process and exorbitant costs to patients.In early 2023, Biogen is looking to give it another go, this time with LEQEMBI (lecanemab-irmb) for the treatment of Alzheimer's disease, which the FDA has approved under its accelerated approval pathway. The drug was jointly developed by Biogen and Tokyo-based pharmaceutical company Eisai (OTC Pink:ESALF,TSE:4523). Intra-Cellular Therapies (NASDAQ:ITCI).Company Profile.Market cap: US$5.96 billion; current share price: US$62.59.As its name suggests, biopharma company Intra-Cellular Therapies is taking an intracellular approach to developing therapies for patients with complex psychiatric and neurologic diseases, including Parkinson’s disease and Alzheimer’s disease.Intra-Cellular Therapies is currently evaluating ITI-1284-ODT-SL, a deuterated form of lumateperone that comes in an oral disintegrating tablet; ITI-1284-ODT-SL is in Phase 1 studies, including drug-drug interaction studies. The company expects to commence Phase 2 clinical trials in agitation in patients with probable Alzheimer’s disease in 2023. 3. Axsome Therapeutics (NASDAQ:AXSM).Company Profile.Market cap: US$3.16 billion; current share price: US$72.64.New York-based Axsome Therapeutics is developing and commercializing a portfolio of novel therapies for disorders of the central nervous system with limited treatment options. Products currently on the market from the biopharmaceutical company include Auvelity, a rapid-acting oral antidepressant for the treatment of major depressive disorder; and Sunosi, a once-daily prescription medicine to treat excessive daytime sleepiness due to obstructive sleep apnea or narcolepsy.Axsome’s product pipeline includes AXS-05, a NMDA receptor antagonist with multimodal activity. It has breakthrough therapy designation from the FDA and is currently in Phase 3 clinical studies for the treatment of Alzheimer’s disease agitation. \"\"\"\n",
    "\n",
    "response = generate(\"You are an equity researcher specialized in the health and pharmaceutical industry. Write a summary for the following article. Focus on the information that will impact investment decisions on the companies mentioned.\\n\" + article)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d685a7e-7614-4383-812c-beecf641a969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
